{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
   
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbFHWCYtUKiG",
        "outputId": "3016c146-d5ba-42dc-87de-5a3d133ee6a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 47 not upgraded.\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.11/dist-packages (0.3.13)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Requirement already satisfied: jiwer in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.1.8)\n",
            "Requirement already satisfied: rapidfuzz>=3.9.7 in /usr/local/lib/python3.11/dist-packages (from jiwer) (3.13.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "# Install Tesseract OCR\n",
        "!apt-get update\n",
        "!apt-get install -y tesseract-ocr\n",
        "# Install pytesseract and Pillow for Python\n",
        "!pip install pytesseract Pillow jiwer transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pytesseract\n",
        "from PIL import Image\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from jiwer import wer\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "import string\n",
        "\n",
        "# Function to calculate Word Error Rate (WER)\n",
        "def calculate_wer(reference, hypothesis):\n",
        "    return wer(reference, hypothesis)\n",
        "\n",
        "# Function to calculate BLEU Score\n",
        "def calculate_bleu(reference, hypothesis):\n",
        "    reference = [reference.split()]  # BLEU expects a list of tokenized reference sentences\n",
        "    hypothesis = hypothesis.split()  # Tokenize the hypothesis\n",
        "    return sentence_bleu(reference, hypothesis)\n",
        "\n",
        "# Function to normalize text (lowercase, remove spaces, and remove punctuation)\n",
        "def normalize_text(text):\n",
        "    \"\"\"\n",
        "    Normalize the text by converting to lowercase, removing punctuation,\n",
        "    and collapsing extra spaces.\n",
        "    \"\"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Remove extra spaces (normalize spaces to a single space)\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    return text\n",
        "\n",
        "# OCR extraction function\n",
        "def ocr_extraction(image_path):\n",
        "    \"\"\"\n",
        "    Perform OCR on the input image and return the extracted text.\n",
        "    \"\"\"\n",
        "    img = Image.open(image_path)\n",
        "    text = pytesseract.image_to_string(img)\n",
        "    return text\n",
        "\n",
        "# Load translation model and tokenizer (English to French)\n",
        "def load_translation_model():\n",
        "    model_name = \"Helsinki-NLP/opus-mt-en-fr\"\n",
        "    model = MarianMTModel.from_pretrained(model_name)\n",
        "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "    return model, tokenizer\n",
        "\n",
        "# Function to translate text using the MarianMT model\n",
        "def translate_text(model, tokenizer, text):\n",
        "    \"\"\"\n",
        "    Translate text from English to French using the MarianMT model.\n",
        "    \"\"\"\n",
        "    # Prepare text for translation\n",
        "    translated = model.generate(**tokenizer.prepare_seq2seq_batch([text], return_tensors=\"pt\"))\n",
        "    translation = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
        "    return translation\n",
        "\n",
        "# OCR evaluation (WER only)\n",
        "def evaluate_ocr(ocr_output, ground_truth_ocr):\n",
        "    \"\"\"\n",
        "    Evaluate OCR output using WER (Word Error Rate).\n",
        "    \"\"\"\n",
        "    ocr_output = normalize_text(ocr_output)\n",
        "    ground_truth_ocr = normalize_text(ground_truth_ocr)\n",
        "    wer_score = calculate_wer(ground_truth_ocr, ocr_output)\n",
        "    return wer_score\n",
        "\n",
        "# Translation evaluation (BLEU only)\n",
        "def evaluate_translation(ocr_output, ground_truth_translation, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Evaluate translation output using BLEU score.\n",
        "    \"\"\"\n",
        "    # Normalize the OCR output and translation ground truth\n",
        "    normalized_ocr = normalize_text(ocr_output)\n",
        "    normalized_translation = normalize_text(ground_truth_translation)\n",
        "\n",
        "    # Translate the OCR text\n",
        "    translated_text = translate_text(model, tokenizer, normalized_ocr)\n",
        "    print(f\"Translated Text: {translated_text}\")\n",
        "\n",
        "    # Tokenize and calculate BLEU score\n",
        "    translated_text = ' '.join(translated_text.split())  # Normalize spacing\n",
        "    ground_truth_translation = ' '.join(normalized_translation.split())  # Normalize spacing\n",
        "    bleu_score = calculate_bleu(ground_truth_translation, translated_text)\n",
        "    return bleu_score\n",
        "\n",
        "\n",
        "def process_image_for_evaluation(image_path, ground_truth_ocr, ground_truth_translation):\n",
        "    # Extract text using OCR\n",
        "    ocr_text = ocr_extraction(image_path)\n",
        "    print(f\"Extracted OCR Text: {ocr_text}\")\n",
        "\n",
        "    # Evaluate OCR accuracy using WER (Word Error Rate)\n",
        "    wer_score = evaluate_ocr(ocr_text, ground_truth_ocr)\n",
        "    print(f\"OCR WER: {wer_score:.4f}\")\n",
        "\n",
        "    # Load translation model\n",
        "    model, tokenizer = load_translation_model()\n",
        "\n",
        "    # Evaluate translation accuracy using BLEU score\n",
        "    bleu_score = evaluate_translation(ocr_text, ground_truth_translation, model, tokenizer)\n",
        "    print(f\"Translation BLEU: {bleu_score:.4f}\")\n",
        "\n",
        "\n",
        "image_path = \"ocr.png\"  # Replace with your  image path\n",
        "ground_truth_ocr =\"This text is easy to extract.\" #add groundtruth text\n",
        "ground_truth_french =\"Ce texte est facile à extraire.\" #add groundtruth french translation\n",
        "process_image_for_evaluation(image_path, ground_truth_ocr, ground_truth_french)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 879,
          "referenced_widgets": [
            "c59b44a087464498a10369d15c19f834",
            "7e18588207a140d9b470a149b8b69ea7",
            "fdc7d459b7524bd7b0c4f4c86219d03e",
            "24ce8e191c524b08925cba308cb8e449",
            "eeea89629e704b678bdb11934b7e7a5e",
            "ff37bc60f114470984c01acd6b731a86",
            "32bd90f936b344918f746d144f1ccb86",
            "5ba78d38bfaa49b78d8a2caac20b1fcf",
            "6679bd67d84f4ff8885014e9d36cba14",
            "592e0b9ff3604c339c7619dc248d2a6b",
            "195069b966de4adfb9ed3ca6201b1066",
            "48f4b1ff954a479c9fd20de6a071c08b",
            "0697009bc74c4e4abd6f6346db9146e1",
            "ffbcada218054f95a0029f3aed87c934",
            "3e2dfb8a87f74387a347fad3420745d1",
            "ce0e99aa2ca54a0aa9f89338a0dc04e0",
            "fd66b274561a49eab15897c183e1a7ef",
            "e6a58c582cff428ead16b9e83167770c",
            "79f8e420c55346589ef8cdf88cedca00",
            "ddf2b19b82e44d83a2ca58cf88b00791",
            "4fe3b92b02a64cb398df146b21c1b67e",
            "6e3c69fcb1e740008b866013692a158c",
            "75da6cd38faf4efdb0db8b043bf89ba5",
            "32057401fe7b4c0aad4d1898f1383fe0",
            "95902e4c3a6b46a9be435069314588a5",
            "82840766b20a4eaea3205b243f32236d",
            "a009ee6628774f068b9a32c3799122fc",
            "794470e085764677a46999ab9d5bab3f",
            "ed3a4e32298f4ec8a80c906aef5040fb",
            "4188ccbea7fd4714be27aa5a71e8dca6",
            "d2e09cc7b66e4300a6fde2a7587ea98b",
            "9dd027ecedcb4a2ebb289daf874d2374",
            "030d806560324bffb5763e1720d387f6",
            "dc69eaae8ae140a7ba6c97d99fd6fd03",
            "23212dc53d8b4fd4b7d5d2195511bdb9",
            "0f2ef3a5b55f4559975354bd2d0b35b3",
            "9ca6a1a5744547c1ad6991026ebbca56",
            "d73a6067e5cb49e2a86df9444064f158",
            "99fcd6ba7a524a04b429d2e48eeeb3dd",
            "b42fe9f06ae449caa9255e4f233c0951",
            "5ffdfa45a73347418b21fa6d39ff7ac8",
            "2290b317987049cca6cec277f52a5d00",
            "de2918f57b4947a18e5dec2c5f6062b4",
            "0a41152a834942938cbfa0292af84d57",
            "783908800bbe4146be557884a8b6f061",
            "673ecbec5b8348c2a354e8ce7293dc86",
            "34df01b00a674c5c8a336772d0cc93c9",
            "2b11deb0da3a427d9fc0e4c52fe9c6c1",
            "eee20d7c54804b51b9e504c4e8e9119a",
            "a8e6e5bf36e44c7ebd9d2550f9bb9421",
            "ecd0f6762ae2459e908ce17532a9fa5d",
            "4c81ad9ae5f94e8aa5b88abcf1eab25c",
            "220d76bfc05e4d038b4c267eeec6a11d",
            "ce9d036c05004c2cb6f9028ceda8fd5a",
            "d6ffd4f9f75a465f9698d627c6139e4c",
            "15dbfde68d42448e8d511537ceba8a0b",
            "7ddd6ca89c834809bfa1615c6807d409",
            "0d202da968974ceab4a111517c3e9cd0",
            "c2addf59f68a41e8920ac6fac935687d",
            "ec5d4863219c4e59967ea9959a5dea06",
            "d836d4eac1bf4ea0a09744283cc31695",
            "b9e2e55935694faeb006fac667c5c00e",
            "59313fde86374b31bbda01e7cec2f056",
            "e52b8d36db47488aa1003c83157ca803",
            "226e892fd5254c908212156684658968",
            "9207ce4818c14cf9b85d46f82beadc25",
            "aec08a0dd7c04b37a8023a10478ec274",
            "91c1411de6464ecab222ed5c731e9942",
            "dc3dbbc7bf0745a1a79374647d625d6a",
            "074970a03fb4442eb02ca7b59b26bbaa",
            "4709f13cc0f348d9b99345291c2a1d29",
            "d2cea843929e454da4ef5442dcc425c8",
            "f6763ecdd6564d82877451dc07a5edb8",
            "a5ce360b565443b5a99d8df113629ae1",
            "6b4f6c9d26e7459887af922eff6a9b53",
            "bae30181972b48c6a3692f24de0468b9",
            "a1ace408912b4ade8928f718121cb8c5",
            "2d5d9177b64e478a9e69e3286338d5d9",
            "899118819a3740a8aa92e96d7e9a324c",
            "c173be23b96c4b05b5838c3458f558ac",
            "2c41b9364bb5410b821624ae166616e5",
            "64f58901cee24d3888c3bb45a8341eaf",
            "ffb921f7aa38456fa581895d625ac497",
            "42cc9955fb1e4859b38eeb30337d2059",
            "fa9debeb291c412ba1e2b8c4da12a2c3",
            "96b3661b0de24442b6a07405607d122f",
            "a7485a0f0efb42d38f37dd95662d5e24",
            "b9bbbbe8cc994856b800e7b09ad61ad0"
          ]
        },
        "id": "rI5vXAOoXXIw",
        "outputId": "3d37977d-ab92-47af-b2b1-1ff3332a1ede"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted OCR Text: This text is\n",
            "easy to extract.\n",
            "\f\n",
            "OCR WER: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.42k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c59b44a087464498a10369d15c19f834"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/301M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "48f4b1ff954a479c9fd20de6a071c08b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/301M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "75da6cd38faf4efdb0db8b043bf89ba5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc69eaae8ae140a7ba6c97d99fd6fd03"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "783908800bbe4146be557884a8b6f061"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "source.spm:   0%|          | 0.00/778k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "15dbfde68d42448e8d511537ceba8a0b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "target.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aec08a0dd7c04b37a8023a10478ec274"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.34M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2d5d9177b64e478a9e69e3286338d5d9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:4106: FutureWarning: \n",
            "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
            "`__call__` method to prepare your inputs and targets.\n",
            "\n",
            "Here is a short example:\n",
            "\n",
            "model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)\n",
            "\n",
            "If you either need to use different keyword arguments for the source and target texts, you should do two calls like\n",
            "this:\n",
            "\n",
            "model_inputs = tokenizer(src_texts, ...)\n",
            "labels = tokenizer(text_target=tgt_texts, ...)\n",
            "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
            "\n",
            "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
            "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
            "\n",
            "  warnings.warn(formatted_warning, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated Text: ce texte est facile à extraire\n",
            "Translation BLEU: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "thVmfQzrfCn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NFu2XfHAa2Cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "sSRY21ESVHHv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
